{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pressing-cloud",
   "metadata": {},
   "source": [
    "# Recommendation system for arXiv manuscripts by Peter Boross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "third-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "import arxiv\n",
    "import sqlite3\n",
    "import urllib.request as libreq\n",
    "import re\n",
    "from collections import Counter\n",
    "import unidecode\n",
    "import itertools\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-transparency",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "educational-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_FLast(authors):\n",
    "    r = []\n",
    "    for author in authors:\n",
    "        if len(author[1]) == 0: r.append(unidecode.unidecode(author[0]))\n",
    "        else: r.append(unidecode.unidecode(author[1][0]+author[0]))    \n",
    "    return ' '.join(r)\n",
    "\n",
    "def get_authors_FLast_arxivapi(authors):\n",
    "    r = []\n",
    "    for authorv in authors:\n",
    "        author = authorv.split(' ')\n",
    "        r.append(unidecode.unidecode(author[0][0]+author[-1]))    \n",
    "    return ' '.join(r)\n",
    "\n",
    "def get_authors_FdotLastcomma(authors):\n",
    "    r = []\n",
    "    for author in authors:\n",
    "        authorv = author.split(' ')\n",
    "        r.append(unidecode.unidecode(' '.join([x[0]+'.' for x in authorv[0:-1]])+' '+authorv[-1]))\n",
    "    return ', '.join(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-string",
   "metadata": {},
   "source": [
    "### Define categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "local-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'cond-mat', 'cond-mat.mes-hall', 'quant-ph', 'cond-mat.supr-con', 'cond-mat.mtrl-sci', 'cond-mat.str-el', 'cond-mat.other'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-glasgow",
   "metadata": {},
   "source": [
    "### Load manuscripts from arXiv JSON by Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "secondary-ecology",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of papers = 315071\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "\n",
    "with open(\"data/arxiv-metadata-oai-snapshot.json\", \"r\") as f:\n",
    "    for l in f:\n",
    "        d = json.loads(l)\n",
    "        if categories & set(d['categories'].split(' ')):\n",
    "            d['authors_FLast'] = get_authors_FLast(d['authors_parsed'])\n",
    "            articles.append(d)\n",
    "\n",
    "articles_df = pd.DataFrame().from_records(articles)\n",
    "\n",
    "print('number of papers =',len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-formula",
   "metadata": {},
   "source": [
    "### Find manuscript of the authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "quantitative-thong",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of papers of the authors = 90\n"
     ]
    }
   ],
   "source": [
    "authors = ['PBoross','LOroszlany','APalyi','JAsboth','GSzechenyi']\n",
    "\n",
    "ids = list(articles_df[articles_df[\"authors_FLast\"].str.contains('|'.join(authors))]['id'])\n",
    "\n",
    "print('number of papers of the authors =',len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-netscape",
   "metadata": {},
   "source": [
    "### Find cited papers by prophy.science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "moderate-samuel",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of cited papers of the authors = 1501\n"
     ]
    }
   ],
   "source": [
    "refs=[]\n",
    "\n",
    "for id in ids:\n",
    "    with libreq.urlopen('https://www.prophy.science/api/arxiv/' + id) as url:\n",
    "        refs1paper = json.loads(url.read())\n",
    "    refs.extend([ref['arxivId'] for ref in refs1paper['references'] if ref['arxivId'] != None])\n",
    "\n",
    "refscounted = sorted(Counter(refs).items(), key=lambda pair: pair[1], reverse=True)\n",
    "refs = [entry[0] for entry in refscounted]\n",
    "counts = [entry[1] for entry in refscounted]\n",
    "refscounteddict = dict(zip(refs, counts))\n",
    "\n",
    "print('number of cited papers of the authors =',len(refscounted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-report",
   "metadata": {},
   "source": [
    "### Make training dataset and write it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "worth-finnish",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of cited papers which in the selected categories = 1498\n",
      "number of non-cited papers which in the selected categories = 14980\n"
     ]
    }
   ],
   "source": [
    "cited_df = articles_df[articles_df['id'].isin(ids+refs)][['abstract','title','authors_FLast','id']].replace(refscounteddict).rename(columns = {'id': 'citation'})\n",
    "cited_df['cited'] = True\n",
    "\n",
    "print('number of cited papers which in the selected categories =',len(cited_df))\n",
    "\n",
    "notcited_df = articles_df[articles_df['id'].isin(refs) == False][['abstract','title','authors_FLast']].sample(n = 10*len(cited_df))\n",
    "notcited_df['citation'] = 0\n",
    "notcited_df['cited'] = False\n",
    "\n",
    "print('number of non-cited papers which in the selected categories =',len(notcited_df))\n",
    "\n",
    "all_df = pd.concat([cited_df, notcited_df])\n",
    "\n",
    "all_df.to_csv('data/traindataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-samoa",
   "metadata": {},
   "source": [
    "### Split to X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dramatic-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_df = pd.read_csv('data/traindataset.csv', index_col=0)\n",
    "\n",
    "X = all_df[['authors_FLast','title','abstract']]\n",
    "y = list(all_df['cited'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-homeless",
   "metadata": {},
   "source": [
    "### Build the model and make cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "refined-labor",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train accuracy =  94.2% test accuracy = 93.1%\ntrain precision =  66.0% test precision = 61.0%\ntrain recall =  74.3% test recall = 64.4%\ntrain roc_auc =  95.8% test roc_auc = 92.4%\n"
     ]
    }
   ],
   "source": [
    "authors_feature = 'authors_FLast'\n",
    "authors_transformer = CountVectorizer(lowercase=False, max_features = 1000)\n",
    "\n",
    "title_feature = 'title'\n",
    "title_transformer = TfidfVectorizer(stop_words='english', ngram_range=(1,3), max_features = 2000)\n",
    "\n",
    "abstract_feature = 'abstract'\n",
    "abstract_transformer = TfidfVectorizer(stop_words='english', ngram_range=(1,3), max_features = 5000)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('authors_FLast', authors_transformer, authors_feature),\n",
    "        ('title', title_transformer, title_feature),\n",
    "        ('abstract', abstract_transformer, abstract_feature)\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', MultinomialNB())])\n",
    "\n",
    "scores = cross_validate(pipeline, X, y, cv=ShuffleSplit(n_splits=5),\n",
    "                        scoring=('accuracy', 'precision', 'recall' , 'roc_auc'),\n",
    "                        return_train_score=True)\n",
    "\n",
    "print('train accuracy = ',\"{:.1f}%\".format(100*np.mean(scores['train_accuracy'])),'test accuracy =',\"{:.1f}%\".format(100*np.mean(scores['test_accuracy'])))\n",
    "print('train precision = ',\"{:.1f}%\".format(100*np.mean(scores['train_precision'])),'test precision =',\"{:.1f}%\".format(100*np.mean(scores['test_precision'])))\n",
    "print('train recall = ',\"{:.1f}%\".format(100*np.mean(scores['train_recall'])),'test recall =',\"{:.1f}%\".format(100*np.mean(scores['test_recall'])))\n",
    "print('train roc_auc = ',\"{:.1f}%\".format(100*np.mean(scores['train_roc_auc'])),'test roc_auc =',\"{:.1f}%\".format(100*np.mean(scores['test_roc_auc'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-angel",
   "metadata": {},
   "source": [
    "### Fit the model and save it"
   ]
  },
  {
   "source": [
    "pipeline.fit(X, y);\n",
    "\n",
    "filename = 'data/model.sav'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": []
  },
  {
   "source": [
    "### Make a query and predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sonic-romance",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of the requested papers =  50\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                   id  published  \\\n",
       "18  http://arxiv.org/abs/2106.01181v1 2021-06-02   \n",
       "17  http://arxiv.org/abs/2106.01188v1 2021-06-02   \n",
       "39  http://arxiv.org/abs/2106.00947v1 2021-06-02   \n",
       "31  http://arxiv.org/abs/2106.01046v1 2021-06-02   \n",
       "37  http://arxiv.org/abs/2106.00994v1 2021-06-02   \n",
       "\n",
       "                                authors_FdotLastcomma  \\\n",
       "18  A. Vekris, J. C. E. Saldana, J. d. Bruijckere,...   \n",
       "17             A. Mishra, P. Simon, T. Hyart, M. Trif   \n",
       "39                       M. Mirzakhani, F. M. Peeters   \n",
       "31           Z. Liu, J. You, B. Gu, S. Maekawa, G. Su   \n",
       "37  A. Widhalm, S. Krehs, D. Siebert, N. L. Sharma...   \n",
       "\n",
       "                                                title  \\\n",
       "18  Asymmetric Little-Parks Oscillations in Full S...   \n",
       "17                           A Yu-Shiba-Rusinov qubit   \n",
       "39         Isolated and hybrid bilayer graphene rings   \n",
       "31  Enhanced spin-orbit coupling and orbital momen...   \n",
       "37  Optoelectronic sampling of ultrafast electric ...   \n",
       "\n",
       "                                             abstract  relevance  \n",
       "18  Little-Parks oscillations of a hollow supercon...   0.844811  \n",
       "17  Magnetic impurities in $s$-wave superconductor...   0.781630  \n",
       "39  Using the continuum model, we investigate the ...   0.706097  \n",
       "31  In atomic physics, the Hund rule says that the...   0.537645  \n",
       "37  In our work, we have engineered low capacitanc...   0.239991  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>published</th>\n      <th>authors_FdotLastcomma</th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>relevance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>http://arxiv.org/abs/2106.01181v1</td>\n      <td>2021-06-02</td>\n      <td>A. Vekris, J. C. E. Saldana, J. d. Bruijckere,...</td>\n      <td>Asymmetric Little-Parks Oscillations in Full S...</td>\n      <td>Little-Parks oscillations of a hollow supercon...</td>\n      <td>0.844811</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>http://arxiv.org/abs/2106.01188v1</td>\n      <td>2021-06-02</td>\n      <td>A. Mishra, P. Simon, T. Hyart, M. Trif</td>\n      <td>A Yu-Shiba-Rusinov qubit</td>\n      <td>Magnetic impurities in $s$-wave superconductor...</td>\n      <td>0.781630</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>http://arxiv.org/abs/2106.00947v1</td>\n      <td>2021-06-02</td>\n      <td>M. Mirzakhani, F. M. Peeters</td>\n      <td>Isolated and hybrid bilayer graphene rings</td>\n      <td>Using the continuum model, we investigate the ...</td>\n      <td>0.706097</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>http://arxiv.org/abs/2106.01046v1</td>\n      <td>2021-06-02</td>\n      <td>Z. Liu, J. You, B. Gu, S. Maekawa, G. Su</td>\n      <td>Enhanced spin-orbit coupling and orbital momen...</td>\n      <td>In atomic physics, the Hund rule says that the...</td>\n      <td>0.537645</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>http://arxiv.org/abs/2106.00994v1</td>\n      <td>2021-06-02</td>\n      <td>A. Widhalm, S. Krehs, D. Siebert, N. L. Sharma...</td>\n      <td>Optoelectronic sampling of ultrafast electric ...</td>\n      <td>In our work, we have engineered low capacitanc...</td>\n      <td>0.239991</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "filename = 'data/model.sav'\n",
    "pipeline = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "days = 1\n",
    "delta = timedelta(days = days)\n",
    "catstr = '+OR+'.join(['cat:'+x for x in categories])\n",
    "client = arxiv.Client()\n",
    "nquery = 500\n",
    "startquery = 0\n",
    "lastquery = nquery\n",
    "latestdate = False\n",
    "predicted_df = pd.DataFrame(columns = ['id','published','authors_FdotLastcomma','authors_FLast', 'title', 'abstract'])\n",
    "\n",
    "while lastquery == nquery:\n",
    "    feedparser = client._parse_feed(url='http://export.arxiv.org/api/query?search_query='+catstr+'&start='+str(startquery)+'&max_results='+str(nquery)+'&sortBy=submittedDate')\n",
    "    if len(feedparser.entries) == 0:\n",
    "        warnings.warn(\"Warning...........arXiv api provides 0 entry\")\n",
    "    lastquery = 0\n",
    "    for entry in feedparser.entries:\n",
    "        if not(latestdate): latestdate = datetime.strptime(entry.published[0:10],'%Y-%m-%d')\n",
    "        if latestdate - datetime.strptime(entry.published[0:10],'%Y-%m-%d') < delta:\n",
    "            lastquery += 1\n",
    "            predicted_df = predicted_df.append({\n",
    "                'id' : entry.id,\n",
    "                'authors_FdotLastcomma' : get_authors_FdotLastcomma([author['name'] for author in entry.authors]),\n",
    "                'authors_FLast' : get_authors_FLast_arxivapi([author['name'] for author in entry.authors]),\n",
    "                'title' : entry.title.rstrip(),\n",
    "                'abstract' : entry.summary.rstrip(),\n",
    "                'published': datetime.strptime(entry.published[0:10],'%Y-%m-%d')\n",
    "                            }, ignore_index = True)\n",
    "    startquery += nquery\n",
    "    time.sleep(5)\n",
    "\n",
    "Xnew = predicted_df[['authors_FLast','title','abstract']]\n",
    "\n",
    "predicted_df['relevance'] = [x[1] for x in pipeline.predict_proba(Xnew)]\n",
    "print('number of the requested papers = ',predicted_df.shape[0])\n",
    "\n",
    "predicted_df[['id','published','authors_FdotLastcomma','title','abstract','relevance']].sort_values(by=['relevance'],ascending=False).head(5)"
   ]
  },
  {
   "source": [
    "### Write predictions into 'manuscripts.db'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/manuscripts.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('CREATE TABLE IF NOT EXISTS manuscripts (id, published, authors, title, abstract, relevance)')\n",
    "conn.commit()\n",
    "\n",
    "tosql_df = predicted_df[['id','published','authors_FdotLastcomma','title','abstract','relevance']].rename(columns={\"authors_FdotLastcomma\": \"authors\"})\n",
    "\n",
    "tosql_df.to_sql('manuscripts', conn, if_exists='replace', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3810jvsc74a57bd074819d5766a1d2ad05cf1ca12736bfed9e6c5a54bd78b045d29372533f44b271",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}