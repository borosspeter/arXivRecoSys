{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pressing-cloud",
   "metadata": {},
   "source": [
    "# Recommendation system for arXiv manuscripts by Peter Boross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "third-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "import arxiv\n",
    "import sqlite3\n",
    "import urllib.request as libreq\n",
    "import re\n",
    "from collections import Counter\n",
    "import unidecode\n",
    "import itertools\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-transparency",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "educational-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_FLast(authors):\n",
    "    r = []\n",
    "    for author in authors:\n",
    "        if len(author[1]) == 0: r.append(unidecode.unidecode(author[0]))\n",
    "        else: r.append(unidecode.unidecode(author[1][0]+author[0]))    \n",
    "    return ' '.join(r)\n",
    "\n",
    "def get_authors_FLast_arxivapi(authors):\n",
    "    r = []\n",
    "    for authorv in authors:\n",
    "        author = authorv.split(' ')\n",
    "        r.append(unidecode.unidecode(author[0][0]+author[-1]))    \n",
    "    return ' '.join(r)\n",
    "\n",
    "def get_authors_FdotLastcomma(authors):\n",
    "    r = []\n",
    "    for author in authors:\n",
    "        authorv = author.split(' ')\n",
    "        r.append(unidecode.unidecode(' '.join([x[0]+'.' for x in authorv[0:-1]])+' '+authorv[-1]))\n",
    "    return ', '.join(r)\n",
    "\n",
    "def progress_bar(relevance):\n",
    "    s = 100-int(100-100*relevance/5)\n",
    "    if s > 14:\n",
    "        if s > 17:\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'orange'\n",
    "    else:\n",
    "        color = 'yellow'\n",
    "    return '|<font color=\"'+color+'\">'+'â–ˆ'*s+'</font>'+'-'*(20-s)+'|'\n",
    "\n",
    "def print_score(scores, type):\n",
    "    extra_space = 9\n",
    "    print('train '+type+' '*(extra_space-len(type))+' = ',\"{:.1f}%\".format(100*np.mean(scores['train_'+type])),'\\ttest '+type+' '*(extra_space-len(type))+' =',\"{:.1f}%\".format(100*np.mean(scores['test_'+type])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-string",
   "metadata": {},
   "source": [
    "### Define categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "local-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'cond-mat', 'cond-mat.mes-hall', 'quant-ph', 'cond-mat.supr-con', 'cond-mat.mtrl-sci', 'cond-mat.str-el', 'cond-mat.other'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-glasgow",
   "metadata": {},
   "source": [
    "### Load manuscripts from arXiv JSON by Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "secondary-ecology",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of papers = 315071\n"
     ]
    }
   ],
   "source": [
    "manuscripts = []\n",
    "\n",
    "with open(\"data/arxiv-metadata-oai-snapshot.json\", \"r\") as f:\n",
    "    for l in f:\n",
    "        d = json.loads(l)\n",
    "        if categories & set(d['categories'].split(' ')):\n",
    "            d['authors_FLast'] = get_authors_FLast(d['authors_parsed'])\n",
    "            manuscripts.append(d)\n",
    "\n",
    "manuscripts_df = pd.DataFrame().from_records(manuscripts)\n",
    "\n",
    "print('number of papers =',len(manuscripts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-formula",
   "metadata": {},
   "source": [
    "### Find manuscript of the authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "quantitative-thong",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of papers of the authors = 90\n"
     ]
    }
   ],
   "source": [
    "authors = ['PBoross','LOroszlany','APalyi','JAsboth','GSzechenyi']\n",
    "\n",
    "ownids = list(manuscripts_df[manuscripts_df[\"authors_FLast\"].str.contains('|'.join(authors))]['id'])\n",
    "\n",
    "print('number of papers of the authors =',len(ownids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-netscape",
   "metadata": {},
   "source": [
    "### Find cited papers by prophy.science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "moderate-samuel",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of cited papers of the authors = 1541\n"
     ]
    }
   ],
   "source": [
    "citedids = ownids.copy()\n",
    "\n",
    "for id in ownids:\n",
    "    with libreq.urlopen('https://www.prophy.science/api/arxiv/' + id) as url:\n",
    "        refs1manuscript = json.loads(url.read())\n",
    "    citedids.extend([ref['arxivId'] for ref in refs1manuscript['references'] if ref['arxivId'] != None])\n",
    "\n",
    "citedidscounted = sorted(Counter(citedids).items(), key=lambda pair: pair[1], reverse=True)\n",
    "citedids = [entry[0] for entry in citedidscounted]\n",
    "counts = [entry[1] for entry in citedidscounted]\n",
    "citedidscounteddict = dict(zip(citedids, counts))\n",
    "\n",
    "print('number of cited papers of the authors =',len(citedidscounted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-report",
   "metadata": {},
   "source": [
    "### Make training dataset and write it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "worth-finnish",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of cited papers which in the selected categories = 1498\n",
      "number of non-cited papers which in the selected categories = 14980\n"
     ]
    }
   ],
   "source": [
    "cited_df = manuscripts_df[manuscripts_df['id'].isin(citedids)][['abstract','title','authors_FLast','id']].replace(citedidscounteddict).rename(columns = {'id': 'citation'})\n",
    "cited_df['cited'] = True\n",
    "\n",
    "print('number of cited papers which in the selected categories =',len(cited_df))\n",
    "\n",
    "notcited_df = manuscripts_df[manuscripts_df['id'].isin(citedids) == False][['abstract','title','authors_FLast']].sample(n = 10*len(cited_df))\n",
    "notcited_df['citation'] = 0\n",
    "notcited_df['cited'] = False\n",
    "\n",
    "print('number of non-cited papers which in the selected categories =',len(notcited_df))\n",
    "\n",
    "all_df = pd.concat([cited_df, notcited_df])\n",
    "\n",
    "all_df.to_csv('data/traindataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-samoa",
   "metadata": {},
   "source": [
    "### Split to X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dramatic-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_df = pd.read_csv('data/traindataset.csv', index_col=0)\n",
    "\n",
    "X = all_df[['authors_FLast','title','abstract']]\n",
    "y = list(all_df['cited'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-homeless",
   "metadata": {},
   "source": [
    "### Build the model and make cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "refined-labor",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train accuracy  =  94.1% \ttest accuracy  = 92.7%\ntrain precision =  65.7% \ttest precision = 59.1%\ntrain recall    =  74.5% \ttest recall    = 64.3%\ntrain f1        =  69.8% \ttest f1        = 61.5%\ntrain roc_auc   =  95.7% \ttest roc_auc   = 91.9%\n"
     ]
    }
   ],
   "source": [
    "authors_feature = 'authors_FLast'\n",
    "authors_transformer = CountVectorizer(lowercase=False, max_features=1000)\n",
    "\n",
    "title_feature = 'title'\n",
    "title_transformer = TfidfVectorizer(stop_words='english', ngram_range=(1,3), max_features=2000)\n",
    "\n",
    "abstract_feature = 'abstract'\n",
    "abstract_transformer = TfidfVectorizer(stop_words='english', ngram_range=(1,4), max_features=5000)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('authors_FLast', authors_transformer, authors_feature),\n",
    "        ('title', title_transformer, title_feature),\n",
    "        ('abstract', abstract_transformer, abstract_feature)\n",
    "    ])\n",
    "\n",
    "classifier = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                             #('balancer', SMOTE()),\n",
    "                             ('classifier', MultinomialNB())])\n",
    "\n",
    "scores = cross_validate(classifier, X, y, cv=StratifiedShuffleSplit(n_splits=5),\n",
    "                        scoring=('accuracy', 'precision', 'recall' , 'f1', 'roc_auc'),\n",
    "                        return_train_score=True)\n",
    "\n",
    "print_score(scores, 'accuracy')\n",
    "print_score(scores, 'precision')\n",
    "print_score(scores, 'recall')\n",
    "print_score(scores, 'f1')\n",
    "print_score(scores, 'roc_auc')"
   ]
  },
  {
   "source": [
    "### GridSearch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=5, random_state=None, test_size=None, train_size=None),\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('authors_FLast',\n",
       "                                                                         CountVectorizer(lowercase=False),\n",
       "                                                                         'authors_FLast'),\n",
       "                                                                        ('title',\n",
       "                                                                         TfidfVectorizer(max_features=20,\n",
       "                                                                                         ngram_range=(1,\n",
       "                                                                                                      3),\n",
       "                                                                                         stop_words='english'),\n",
       "                                                                         'title'),\n",
       "                                                                        ('abstract',\n",
       "                                                                         TfidfVectorizer(max_features=50,\n",
       "                                                                                         ngram_range=(1,\n",
       "                                                                                                      3),\n",
       "                                                                                         stop_words='english'),\n",
       "                                                                         'abstract')])),\n",
       "                                       ('classifier', MultinomialNB())]),\n",
       "             param_grid={'preprocessor__authors_FLast__max_features': [10, 20,\n",
       "                                                                       30]},\n",
       "             return_train_score=True, scoring='f1')"
      ]
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "authors_feature = 'authors_FLast'\n",
    "authors_transformer = CountVectorizer(lowercase=False)\n",
    "\n",
    "title_feature = 'title'\n",
    "title_transformer = TfidfVectorizer(stop_words='english', ngram_range=(1,3), max_features=20)\n",
    "\n",
    "abstract_feature = 'abstract'\n",
    "abstract_transformer = TfidfVectorizer(stop_words='english', ngram_range=(1,3), max_features=50)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('authors_FLast', authors_transformer, authors_feature),\n",
    "        ('title', title_transformer, title_feature),\n",
    "        ('abstract', abstract_transformer, abstract_feature)\n",
    "    ])\n",
    "\n",
    "param_grids = {'preprocessor__authors_FLast__max_features':[10,20,30]}\n",
    "\n",
    "classifier = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            #('balancer', RandomOverSampler()),\n",
    "                            ('classifier', MultinomialNB())])\n",
    "\n",
    "gridsearch = GridSearchCV(classifier, param_grids, cv=ShuffleSplit(n_splits=5),\n",
    "                          scoring=('f1'),\n",
    "                          return_train_score=True)\n",
    "\n",
    "gridsearch.fit(X,y)"
   ]
  },
  {
   "source": [
    "classifier.fit(X, y);\n",
    "\n",
    "filename = 'data/model.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 163,
   "outputs": []
  },
  {
   "source": [
    "### Make a query and predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "sonic-romance",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of the requested papers =  390\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                    id  published  \\\n",
       "81   http://arxiv.org/abs/2106.03082v1 2021-06-06   \n",
       "260  http://arxiv.org/abs/2106.01391v1 2021-06-02   \n",
       "42   http://arxiv.org/abs/2106.03435v1 2021-06-07   \n",
       "246  http://arxiv.org/abs/2106.01576v1 2021-06-03   \n",
       "324  http://arxiv.org/abs/2106.00800v2 2021-06-01   \n",
       "\n",
       "                                 authors_FdotLastcomma  \\\n",
       "81   M. T. Madzik, S. Asaad, A. Youssry, B. Joecker...   \n",
       "260                          D. Buterakos, S. D. Sarma   \n",
       "42   V. L. Muller, Y. Yan, O. Kashuba, B. Trauzette...   \n",
       "246  K. Kuroyama, S. Matsuo, J. Muramoto, S. Yabuna...   \n",
       "324                                B. Mera, N. Goldman   \n",
       "\n",
       "                                                 title  \\\n",
       "81   Precision tomography of a three-qubit electron...   \n",
       "260  Spin-Valley Qubit Dynamics In Exchange Coupled...   \n",
       "42   Electron-hole scattering limited transport of ...   \n",
       "246  Real-time observation of charge-spin cooperati...   \n",
       "324  Relating the topology of Dirac Hamiltonians to...   \n",
       "\n",
       "                                              abstract  relevance  \n",
       "81   Nuclear spins were among the first physical pl...   1.000000  \n",
       "260  The presence of valley states is a significant...   0.996651  \n",
       "42   We experimentally investigate the effect of el...   0.988116  \n",
       "246  Quantum dots are recognized as a suitable plat...   0.980173  \n",
       "324  Quantum geometry has emerged as a central and ...   0.936935  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>published</th>\n      <th>authors_FdotLastcomma</th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>relevance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>81</th>\n      <td>http://arxiv.org/abs/2106.03082v1</td>\n      <td>2021-06-06</td>\n      <td>M. T. Madzik, S. Asaad, A. Youssry, B. Joecker...</td>\n      <td>Precision tomography of a three-qubit electron...</td>\n      <td>Nuclear spins were among the first physical pl...</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>260</th>\n      <td>http://arxiv.org/abs/2106.01391v1</td>\n      <td>2021-06-02</td>\n      <td>D. Buterakos, S. D. Sarma</td>\n      <td>Spin-Valley Qubit Dynamics In Exchange Coupled...</td>\n      <td>The presence of valley states is a significant...</td>\n      <td>0.996651</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>http://arxiv.org/abs/2106.03435v1</td>\n      <td>2021-06-07</td>\n      <td>V. L. Muller, Y. Yan, O. Kashuba, B. Trauzette...</td>\n      <td>Electron-hole scattering limited transport of ...</td>\n      <td>We experimentally investigate the effect of el...</td>\n      <td>0.988116</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>http://arxiv.org/abs/2106.01576v1</td>\n      <td>2021-06-03</td>\n      <td>K. Kuroyama, S. Matsuo, J. Muramoto, S. Yabuna...</td>\n      <td>Real-time observation of charge-spin cooperati...</td>\n      <td>Quantum dots are recognized as a suitable plat...</td>\n      <td>0.980173</td>\n    </tr>\n    <tr>\n      <th>324</th>\n      <td>http://arxiv.org/abs/2106.00800v2</td>\n      <td>2021-06-01</td>\n      <td>B. Mera, N. Goldman</td>\n      <td>Relating the topology of Dirac Hamiltonians to...</td>\n      <td>Quantum geometry has emerged as a central and ...</td>\n      <td>0.936935</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 164
    }
   ],
   "source": [
    "filename = 'data/model.sav'\n",
    "classifier = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "days = 7\n",
    "delta = timedelta(days = days)\n",
    "catstr = '+OR+'.join(['cat:'+x for x in categories])\n",
    "client = arxiv.Client()\n",
    "nquery = 500\n",
    "startquery = 0\n",
    "lastquery = nquery\n",
    "latestdate = False\n",
    "predicted_df = pd.DataFrame(columns = ['id','published','authors_FdotLastcomma','authors_FLast', 'title', 'abstract'])\n",
    "\n",
    "while lastquery == nquery:\n",
    "    feedparser = client._parse_feed(url='http://export.arxiv.org/api/query?search_query='+catstr+'&start='+str(startquery)+'&max_results='+str(nquery)+'&sortBy=submittedDate')\n",
    "    if len(feedparser.entries) == 0:\n",
    "        warnings.warn(\"Warning...........arXiv api provides 0 entry\")\n",
    "    lastquery = 0\n",
    "    for entry in feedparser.entries:\n",
    "        if not(latestdate): latestdate = datetime.strptime(entry.published[0:10],'%Y-%m-%d')\n",
    "        if latestdate - datetime.strptime(entry.published[0:10],'%Y-%m-%d') < delta:\n",
    "            lastquery += 1\n",
    "            predicted_df = predicted_df.append({\n",
    "                'id' : entry.id,\n",
    "                'authors_FdotLastcomma' : get_authors_FdotLastcomma([author['name'] for author in entry.authors]),\n",
    "                'authors_FLast' : get_authors_FLast_arxivapi([author['name'] for author in entry.authors]),\n",
    "                'title' : entry.title.replace('\\n', ' '),\n",
    "                'abstract' : entry.summary.replace('\\n', ' '),\n",
    "                'published': datetime.strptime(entry.published[0:10],'%Y-%m-%d')\n",
    "                            }, ignore_index = True)\n",
    "    startquery += nquery\n",
    "    time.sleep(5)\n",
    "\n",
    "Xnew = predicted_df[['authors_FLast','title','abstract']]\n",
    "\n",
    "predicted_df['relevance'] = [x[1] for x in classifier.predict_proba(Xnew)]\n",
    "print('number of the requested papers = ',predicted_df.shape[0])\n",
    "\n",
    "predicted_df[['id','published','authors_FdotLastcomma','title','abstract','relevance']].sort_values(by=['relevance'],ascending=False).head(5)"
   ]
  },
  {
   "source": [
    "### Write predictions into 'manuscripts.db'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tosql_df = predicted_df[['id','published','authors_FdotLastcomma','title','abstract','relevance']].rename(columns = {\"authors_FdotLastcomma\": \"authors\"})\n",
    "\n",
    "conn = sqlite3.connect('data/manuscripts.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('CREATE TABLE IF NOT EXISTS manuscripts (id, published, authors, title, abstract, relevance)')\n",
    "conn.commit()\n",
    "\n",
    "tosql_df.to_sql('manuscripts', conn, if_exists = 'replace', index = False)"
   ]
  },
  {
   "source": [
    "### Write predictions into 'manuscripts.html'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tohtml_df = predicted_df[['id','published','authors_FdotLastcomma','title','abstract','relevance']].rename(columns = {\"authors_FdotLastcomma\": \"authors\"})\n",
    "\n",
    "day = False\n",
    "html = '<ul>\\n'\n",
    "html += '<hr>\\n'\n",
    "for idx, row in tohtml_df[tohtml_df['relevance']>0.5].sort_values(by=['published','relevance'],ascending=False).iterrows():\n",
    "    if day != row['published']:\n",
    "        html += '<div class=\"date\">'+row['published'].strftime('%-d %B, %Y')+'</div>\\n'\n",
    "        html += '<hr>\\n'\n",
    "        day = row['published']\n",
    "    html += '<li>\\n'\n",
    "    html += '<a href=\"'+row['id']+'\">arXiv:'+row['id'].split('http://arxiv.org/abs/')[-1][:-2]+'</a>\\n'\n",
    "    html += '<div class=\"relevance\"><b>Relevance:</b>'\n",
    "    html += '<font style=\"font-family:courier, monospace\">'+progress_bar(row['relevance'])+'</font>'\n",
    "    html += str(round(100*row['relevance'],1))+'%</div>\\n'\n",
    "    html += '<div class=\"title\"><b>Title:</b> '+row['title']+'</div>\\n'\n",
    "    html += '<div class=\"authors_head\"><b>Authors:</div></b> '\n",
    "    html += '<div class=\"authors\"><i>'+row['authors']+'</i></div>\\n'\n",
    "    html += '<div class=\"abstract_head\"><b>Abstract:</b></div>\\n'\n",
    "    html += '<div class=\"abstract\">'+row['abstract']+'</div>\\n'\n",
    "    html += '<br>\\n'\n",
    "    html += '</li>\\n'\n",
    "    html += '<hr>\\n'\n",
    "html += '</ul>'\n",
    "\n",
    "with open(\"data/template.html\", \"r\") as file:\n",
    "    template = file.read()\n",
    "\n",
    "html = template.replace(\"***\", html)\n",
    "\n",
    "with open(\"data/manuscripts.html\", \"w\") as file:\n",
    "        file.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3810jvsc74a57bd074819d5766a1d2ad05cf1ca12736bfed9e6c5a54bd78b045d29372533f44b271",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}